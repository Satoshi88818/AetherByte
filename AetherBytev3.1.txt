#!/usr/bin/env python3
"""
Aether Byte v3.1 ‚Äî Fixed & Ready Bidirectional Multimodal Tiny Byte-Level AGI
(Feb 2026) ‚Äî All critical bugs fixed
1. Nucleus top-p sampling now correct
2. Device reference fixed
3. Class order fixed (no more NameError)
4. AE now pre-trained ‚Üí real PNG I/O
"""

import argparse
import os
import logging
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
import requests
from io import BytesIO
from PIL import Image
import json
import time
import random
from typing import List, Tuple
import numpy as np
import torch.nn.functional as F
import re
import zipfile

logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')
logger = logging.getLogger(__name__)

# ====================== CONFIGS v3.1 ======================
class ModelConfig:
    def __init__(self):
        self.vocab_size = 512
        self.pad_id = 511
        self.bos_id = 256
        self.eos_id = 257
        self.user_id = 258
        self.assist_id = 259
        self.thought_id = 260
        self.action_id = 261
        self.img_start = 262
        self.img_end = 263
        self.tool_start = 264
        self.tool_end = 265
        self.tool_result = 266

        self.embed_dim = 512
        self.num_heads = 16
        self.num_layers = 8
        self.max_seq_len = 8192
        self.dropout = 0.1
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.image_size = 64
        self.num_vision_tokens = 48
        self.latent_dim = 128
        self.head_dim = self.embed_dim // self.num_heads
        self.use_compile = True

class TrainingConfig:
    def __init__(self):
        self.learning_rate = 3e-4
        self.batch_size = 6
        self.epochs = 20
        self.gradient_accumulation_steps = 4
        self.grad_clip_norm = 1.0
        self.checkpoint_dir = './checkpoints'

class DataConfig:
    def __init__(self):
        self.pico_banana_jsonl_url = "https://ml-site.cdn-apple.com/datasets/pico-banana-300k/nb/jsonl/sft.jsonl"
        self.max_samples = 10000
        self.multimodal_fraction = 0.5

class AgentConfig:
    def __init__(self):
        self.max_inner_steps = 8
        self.memory_capacity = 8192
        self.retrieve_k = 4

class AetherByteConfig:
    def __init__(self):
        self.model = ModelConfig()
        self.training = TrainingConfig()
        self.data = DataConfig()
        self.agent = AgentConfig()

cfg = AetherByteConfig()

# ====================== TINY AUTOENCODER ======================
class TinyAutoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 4, stride=2, padding=1), nn.ReLU(),
            nn.Conv2d(32, 64, 4, stride=2, padding=1), nn.ReLU(),
            nn.Conv2d(64, 32, 4, stride=2, padding=1), nn.ReLU(),
            nn.Flatten(),
            nn.Linear(32 * 8 * 8, cfg.model.latent_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(cfg.model.latent_dim, 32 * 8 * 8),
            nn.Unflatten(1, (32, 8, 8)),
            nn.ConvTranspose2d(32, 64, 4, stride=2, padding=1), nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1), nn.ReLU(),
            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),
            nn.Sigmoid()
        )

    def encode(self, x: torch.Tensor) -> bytes:
        with torch.no_grad():
            latent = self.encoder(x)
            return (latent.clamp(0, 1) * 255).byte().flatten().cpu().numpy().tobytes()

    def decode(self, latent_bytes: bytes) -> Image.Image:
        latent = torch.from_numpy(np.frombuffer(latent_bytes, dtype=np.uint8)).float().view(1, cfg.model.latent_dim) / 255.0
        latent = latent.to(cfg.model.device)
        with torch.no_grad():
            recon = self.decoder(latent)
            img = (recon * 255).clamp(0, 255).byte().squeeze(0).permute(1, 2, 0).cpu().numpy()
            return Image.fromarray(img)

# ====================== UTILITIES ======================
def pack_segment(start_id: int, data: bytes, end_id: int) -> List[int]:
    length = len(data)
    seq = [start_id]
    seq.extend(length.to_bytes(4, 'big'))
    seq.extend(list(data))
    seq.append(end_id)
    return seq

def unpack_generated(generated: List[int]) -> Tuple[bytes, List[bytes]]:
    text_parts = []
    images = []
    i = 0
    while i < len(generated):
        tok = generated[i]
        if tok == cfg.model.img_start and i + 5 < len(generated):
            try:
                length = int.from_bytes(bytes(generated[i+1:i+5]), 'big')
                end_pos = i + 5 + length
                if end_pos < len(generated) and generated[end_pos] == cfg.model.img_end:
                    img_data = bytes(generated[i+5:end_pos])
                    images.append(img_data)
                    text_parts.append(b"[üñºÔ∏è PNG IMAGE]")
                    i = end_pos + 1
                    continue
            except:
                pass
        elif tok == cfg.model.tool_start and i + 5 < len(generated):
            try:
                length = int.from_bytes(bytes(generated[i+1:i+5]), 'big')
                end_pos = i + 5 + length
                if end_pos < len(generated) and generated[end_pos] == cfg.model.tool_end:
                    tool_data = bytes(generated[i+5:end_pos])
                    text_parts.append(b"üîß TOOL: " + tool_data)
                    i = end_pos + 1
                    continue
            except:
                pass
        if tok < 256:
            text_parts.append(bytes([tok]))
        else:
            text_parts.append(b"[SPECIAL]")
        i += 1

    decoded = b"".join(text_parts)
    decoded = re.sub(rb'(?i)\b(rm -rf|sudo|format|delete|shutdown|exploit|harm)\b', b'[CENSORED]', decoded)
    return decoded, images

# ====================== TRANSFORMER BLOCKS ======================
class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight

class SwiGLUFFN(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hidden = dim * 4
        self.w1 = nn.Linear(dim, hidden, bias=False)
        self.w2 = nn.Linear(dim, hidden, bias=False)
        self.w3 = nn.Linear(hidden, dim, bias=False)
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        return self.dropout(self.w3(F.silu(self.w1(x)) * self.w2(x)))

class RotaryEmbedding(nn.Module):
    def __init__(self, dim, max_position_embeddings=8192, base=10000):
        super().__init__()
        self.dim = dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer("inv_freq", inv_freq)
        self._set_cos_sin_cache(max_position_embeddings)

    def _set_cos_sin_cache(self, seq_len):
        t = torch.arange(seq_len, dtype=self.inv_freq.dtype, device=self.inv_freq.device)
        freqs = torch.outer(t, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos_cached", emb.cos(), persistent=False)
        self.register_buffer("sin_cached", emb.sin(), persistent=False)

    def forward(self, seq_len: int, offset: int = 0):
        if offset + seq_len > self.cos_cached.shape[0]:
            self._set_cos_sin_cache(offset + seq_len)
        return self.cos_cached[offset:offset+seq_len], self.sin_cached[offset:offset+seq_len]

def rotate_half(x):
    x1 = x[..., :x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(q, k, cos, sin):
    cos = cos.unsqueeze(0).unsqueeze(0)
    sin = sin.unsqueeze(0).unsqueeze(0)
    q = (q * cos) + (rotate_half(q) * sin)
    k = (k * cos) + (rotate_half(k) * sin)
    return q, k

class SelfAttention(nn.Module):
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.num_heads = config.num_heads
        self.head_dim = config.head_dim
        self.embed_dim = config.embed_dim
        self.qkv_proj = nn.Linear(self.embed_dim, 3 * self.embed_dim, bias=False)
        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)
        self.rotary_emb = RotaryEmbedding(self.head_dim, max_position_embeddings=config.max_seq_len)
        self.dropout = config.dropout

    def forward(self, x, padding_mask=None, past_key_value=None, use_cache=False):
        B, T, C = x.shape
        qkv = self.qkv_proj(x)
        q, k, v = torch.chunk(qkv, 3, dim=-1)
        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)

        past_len = past_key_value[0].shape[2] if past_key_value is not None else 0
        cos, sin = self.rotary_emb(T, past_len)
        q, k = apply_rotary_pos_emb(q, k, cos, sin)

        if past_key_value is not None:
            k = torch.cat([past_key_value[0], k], dim=2)
            v = torch.cat([past_key_value[1], v], dim=2)
        present_key_value = (k, v) if use_cache else None

        attn_mask = padding_mask[:, None, None, :] if padding_mask is not None else None
        attn_output = F.scaled_dot_product_attention(
            q, k, v, attn_mask=attn_mask,
            dropout_p=self.dropout if self.training else 0.0, is_causal=True
        )
        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)
        return self.out_proj(attn_output), present_key_value

class CrossAttention(nn.Module):
    def __init__(self, embed_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.kv_proj = nn.Linear(embed_dim, 2 * embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)   # ‚Üê added for stability

    def forward(self, x, vision_context):
        B, T, C = x.shape
        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        kv = self.kv_proj(vision_context).view(B, vision_context.shape[1], 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        k, v = kv[0], kv[1]
        attn = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0)
        attn = attn.transpose(1, 2).contiguous().view(B, T, C)
        return self.out_proj(attn)

class TransformerLayer(nn.Module):
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.self_attn = SelfAttention(config)
        self.cross_attn = CrossAttention(config.embed_dim, config.num_heads)
        self.norm1 = RMSNorm(config.embed_dim)
        self.norm_cross = RMSNorm(config.embed_dim)
        self.norm2 = RMSNorm(config.embed_dim)
        self.ffn = SwiGLUFFN(config.embed_dim)
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x, padding_mask=None, past_key_value=None, use_cache=False, vision_context=None):
        residual = x
        x = self.norm1(x)
        attn_out, present = self.self_attn(x, padding_mask, past_key_value, use_cache)
        x = residual + self.dropout(attn_out)

        if vision_context is not None:
            residual = x
            x = self.norm_cross(x)
            cross_out = self.cross_attn(x, vision_context)
            x = residual + self.dropout(cross_out)

        residual = x
        x = self.norm2(x)
        x = residual + self.ffn(x)
        return x, present

# ====================== VISION ENCODER (moved up) ======================
class TinyPatchViT(nn.Module):
    def __init__(self):
        super().__init__()
        self.num_tokens = cfg.model.num_vision_tokens
        embed_dim = 192
        self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=8, stride=8)
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.randn(1, 65, embed_dim))
        encoder_layer = nn.TransformerEncoderLayer(embed_dim, 8, 768, dropout=0.1, activation='gelu', batch_first=True, norm_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=3)
        self.proj = nn.Linear(embed_dim, cfg.model.embed_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B = x.shape[0]
        patches = self.patch_embed(x).flatten(2).transpose(1, 2)
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat((cls_tokens, patches), dim=1) + self.pos_embed
        x = self.transformer(x)
        features = x[:, 1:]
        pooled = F.adaptive_avg_pool1d(features.transpose(1, 2), self.num_tokens).transpose(1, 2)
        return self.proj(pooled)

# ====================== BYTE TRANSFORMER ======================
class ByteTransformer(nn.Module):
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        self.embedding = nn.Embedding(config.vocab_size, config.embed_dim)
        self.ae = TinyAutoencoder()
        self.vision_encoder = TinyPatchViT()
        self.layers = nn.ModuleList([TransformerLayer(config) for _ in range(config.num_layers)])
        self.norm = RMSNorm(config.embed_dim)
        self.head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)

    def forward(self, x, padding_mask=None, targets=None, past_key_values=None, use_cache=False, vision_context=None):
        B, T = x.shape
        emb = self.embedding(x)
        out = emb
        present_key_values = [] if use_cache else None
        for i, layer in enumerate(self.layers):
            past = past_key_values[i] if past_key_values else None
            out, kv = layer(out, padding_mask, past, use_cache, vision_context)
            if use_cache:
                present_key_values.append(kv)
        out = self.norm(out)
        logits = self.head(out)

        loss = None
        if targets is not None:
            shift_logits = logits[:, :-1].contiguous().view(-1, self.config.vocab_size)
            shift_targets = targets[:, 1:].contiguous().view(-1)
            loss = F.cross_entropy(shift_logits, shift_targets, ignore_index=self.config.pad_id)
        return logits, loss, present_key_values

# ====================== MEMORY & SAMPLING (fixed) ======================
class VectorMemory:
    def __init__(self, model):
        self.model = model
        self.capacity = cfg.agent.memory_capacity
        self.keys = torch.zeros((self.capacity, cfg.model.embed_dim), dtype=torch.float16, device='cpu')
        self.values = [None] * self.capacity
        self.ptr = 0
        self.size = 0

    def add(self, tokens: List[int]):
        if not tokens: return
        with torch.no_grad():
            emb = self.model.embedding(torch.tensor(tokens, device=cfg.model.device)).mean(dim=0).cpu().to(torch.float16)
        self.keys[self.ptr] = emb
        self.values[self.ptr] = {"text": bytes([t for t in tokens if t < 256]).decode('utf-8', errors='ignore')[:300], "time": time.time()}
        self.ptr = (self.ptr + 1) % self.capacity
        if self.size < self.capacity: self.size += 1

    def retrieve(self, query_tokens: List[int], k=None):
        k = k or cfg.agent.retrieve_k
        if self.size == 0: return []
        with torch.no_grad():
            q = self.model.embedding(torch.tensor(query_tokens, device=cfg.model.device)).mean(dim=0).cpu().to(torch.float16)
        sim = F.cosine_similarity(q.unsqueeze(0), self.keys[:self.size])
        topk = sim.topk(min(k, self.size))
        return [self.values[i] for i in topk.indices.tolist()]

@torch.no_grad()
def nucleus_sample(model, prompt_tokens: List[int], max_new: int = 1536, temp: float = 0.87, top_p: float = 0.93, past_key_values=None, vision_context=None):
    model.eval()
    device = cfg.model.device
    eos_id = cfg.model.eos_id
    generated = []

    # Fixed input handling with cache
    if past_key_values is None or not prompt_tokens:
        input_ids = torch.tensor([prompt_tokens], dtype=torch.long).to(device)
    else:
        input_ids = torch.tensor([[prompt_tokens[-1]]], dtype=torch.long).to(device)

    logits, _, past_key_values = model(input_ids, past_key_values=past_key_values, use_cache=True, vision_context=vision_context)
    logits = logits[0, -1] / temp

    for _ in range(max_new):
        # FIXED TOP-P NUCLEUS
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        sorted_indices_to_remove = cumulative_probs > top_p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = False

        logits[sorted_indices[sorted_indices_to_remove]] = float('-inf')

        probs = F.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, 1).item()

        generated.append(next_token)
        input_ids = torch.tensor([[next_token]], device=device)
        logits, _, past_key_values = model(input_ids, past_key_values=past_key_values, use_cache=True, vision_context=vision_context)
        logits = logits[0, -1] / temp

        if next_token == eos_id:
            break
    return generated, past_key_values

class SegmentAwareContext:
    def __init__(self, max_len: int):
        self.max_len = max_len
        self.buffer: List[int] = []
        self.segment_ends = {cfg.model.img_end, cfg.model.tool_end, cfg.model.eos_id, cfg.model.assist_id, cfg.model.action_id}

    def append(self, tokens: List[int]):
        self.buffer.extend(tokens)
        self._trim_if_needed()

    def get(self) -> List[int]:
        return self.buffer[:]

    def _trim_if_needed(self):
        if len(self.buffer) <= self.max_len: return
        excess = len(self.buffer) - self.max_len
        trim_idx = 0
        for i in range(len(self.buffer) - 1, excess - 1, -1):
            if self.buffer[i] in self.segment_ends:
                trim_idx = i + 1
                break
        if trim_idx == 0: trim_idx = excess
        self.buffer = self.buffer[trim_idx:]

# ====================== TOOLS ======================
def web_search(query: str) -> bytes:
    try:
        r = requests.get(f"https://api.duckduckgo.com/?q={requests.utils.quote(query)}&format=json",
                         headers={"User-Agent": "Aether Byte/3.1"}, timeout=8)
        data = r.json()
        abstract = data.get("AbstractText") or data.get("Abstract", "")
        if abstract:
            return f"Abstract: {abstract}\nSource: {data.get('AbstractURL')}".encode()
        topics = [t.get("Text", "") for t in data.get("RelatedTopics", [])[:5]]
        return ("Top results:\n" + "\n".join(topics)).encode()[:2000]
    except:
        return b"Web search unavailable"

def browse_page(url: str) -> bytes:
    try:
        r = requests.get(url, headers={"User-Agent": "Aether Byte/3.1"}, timeout=10)
        r.raise_for_status()
        text = re.sub(r'<[^>]+>', ' ', r.text)
        text = re.sub(r'\s+', ' ', text)[:1800]
        return f"Page content from {url}:\n{text}".encode()
    except Exception as e:
        return f"Browse error: {e}".encode()

ALLOWED_TOOLS = {"calc", "time", "echo", "web_search", "browse_page"}

def execute_tool(tool_call_bytes: bytes) -> bytes:
    try:
        call_str = tool_call_bytes.decode('utf-8', errors='ignore').strip()
        d = json.loads(call_str)
        tool_name = d.get('name')
        params = d.get('parameters', {})
        if tool_name not in ALLOWED_TOOLS:
            return b"Invalid tool"
        if tool_name == "web_search":
            return web_search(params.get('query', ''))
        if tool_name == "browse_page":
            return browse_page(params.get('url', ''))
        if tool_name == "calc":
            try: return str(eval(params.get('expression', '0'))).encode()
            except: return b"Calc error"
        if tool_name == "time":
            return time.strftime("%Y-%m-%d %H:%M:%S").encode()
        if tool_name == "echo":
            return params.get('text', '').encode()
        return b"Tool not implemented"
    except:
        return b"Tool call invalid"

# ====================== AGENT ======================
class AetherAgent:
    def __init__(self, model):
        self.model = model
        self.context = SegmentAwareContext(cfg.model.max_seq_len)
        self.memory = VectorMemory(model)
        self.img_counter = 0
        self.past_key_values = None
        self.vision_context = None
        os.makedirs("generated_images", exist_ok=True)

    def add_to_context(self, tokens: List[int], update_vision=False):
        if not tokens: return
        token_tensor = torch.tensor([tokens], device=cfg.model.device)
        with torch.no_grad():
            _, _, self.past_key_values = self.model(token_tensor, past_key_values=self.past_key_values, use_cache=True,
                                                    vision_context=self.vision_context if update_vision else None)
        self.context.append(tokens)

    def encode_image_input(self, image_url: str) -> Tuple[List[int], torch.Tensor]:
        try:
            r = requests.get(image_url, timeout=15)
            img = Image.open(BytesIO(r.content)).convert('RGB').resize((cfg.model.image_size, cfg.model.image_size))
            img_tensor = (torch.from_numpy(np.array(img)).permute(2, 0, 1).unsqueeze(0).float() / 255.0).to(cfg.model.device)
            latent_bytes = self.model.ae.encode(img_tensor)
            vision_emb = self.model.vision_encoder(img_tensor)
            return pack_segment(cfg.model.img_start, latent_bytes, cfg.model.img_end), vision_emb
        except Exception as e:
            logger.error(f"Image input failed: {e}")
            return list(f"[IMAGE ERROR: {e}]".encode('utf-8')), None

    def run(self):
        print("\n=== Aether Byte v3.1 ‚Äî Fixed & Ready ===")
        print("Image URL or text ‚Üí 'exit' to quit\n")
        self.add_to_context([cfg.model.bos_id])

        while True:
            user_input = input("You: ").strip()
            if user_input.lower() in {'exit', 'quit'}:
                break

            self.add_to_context([cfg.model.user_id])
            is_image = re.match(r'^https?://.*\.(png|jpg|jpeg|webp|gif)$', user_input, re.I)
            if is_image:
                img_segment, vision_emb = self.encode_image_input(user_input)
                if vision_emb is not None:
                    self.vision_context = vision_emb
                self.add_to_context(img_segment, update_vision=True)
                print("üñºÔ∏è Image encoded")
            else:
                self.add_to_context(list(user_input.encode('utf-8')))

            recalled = self.memory.retrieve(list(user_input.encode('utf-8')))
            if recalled:
                self.add_to_context([cfg.model.thought_id])
                self.add_to_context(list(b"[MEMORY] " + recalled[0]["text"].encode()[:100]))

            self.add_to_context([cfg.model.thought_id])

            for _ in range(cfg.agent.max_inner_steps):
                new_tokens, self.past_key_values = nucleus_sample(self.model, self.context.get(), max_new=1024, temp=0.82, top_p=0.91,
                                                                   past_key_values=self.past_key_values, vision_context=self.vision_context)
                self.context.append(new_tokens)

                tool_payload = None
                try:
                    start_idx = next((i for i, t in enumerate(new_tokens) if t == cfg.model.tool_start), None)
                    if start_idx is not None and start_idx + 5 < len(new_tokens):
                        length = int.from_bytes(bytes(new_tokens[start_idx+1:start_idx+5]), 'big')
                        end_idx = start_idx + 5 + length
                        if end_idx < len(new_tokens) and new_tokens[end_idx] == cfg.model.tool_end:
                            tool_payload = bytes(new_tokens[start_idx + 5 : end_idx])
                except:
                    pass

                if tool_payload:
                    result_bytes = execute_tool(tool_payload)
                    self.add_to_context([cfg.model.tool_result])
                    self.add_to_context(list(result_bytes))
                    self.add_to_context([cfg.model.action_id])
                    self.memory.add(new_tokens + list(result_bytes))
                else:
                    break

            self.add_to_context([cfg.model.assist_id])
            final_tokens, self.past_key_values = nucleus_sample(self.model, self.context.get(), max_new=2048, temp=0.87, top_p=0.93,
                                                                past_key_values=self.past_key_values, vision_context=self.vision_context)
            self.context.append(final_tokens)
            self.memory.add(final_tokens)

            decoded, image_latents = unpack_generated(final_tokens)

            for latent_bytes in image_latents:
                self.img_counter += 1
                try:
                    img = self.model.ae.decode(latent_bytes)
                    path = f"generated_images/output_{self.img_counter}_{int(time.time())}.png"
                    img.save(path)
                    print(f"üñºÔ∏è Saved ‚Üí {path}")
                except Exception as e:
                    print(f"Decode failed: {e}")

            print("Aether:", decoded.decode('utf-8', errors='ignore').strip())

# ====================== DATASET ======================
class AetherByteDataset(Dataset):
    def __init__(self, model):
        self.data = []
        self.ae = model.ae
        self.device = cfg.model.device
        max_samples = cfg.data.max_samples
        multimodal_samples = int(max_samples * cfg.data.multimodal_fraction)

        # COCO multimodal
        if not os.path.exists('val2017'):
            logger.info("Downloading COCO val2017...")
            r = requests.get('http://images.cocodataset.org/zips/val2017.zip')
            with open('val2017.zip', 'wb') as f: f.write(r.content)
            with zipfile.ZipFile('val2017.zip', 'r') as z: z.extractall('.')
            os.remove('val2017.zip')

        if not os.path.exists('annotations'):
            logger.info("Downloading COCO annotations...")
            r = requests.get('http://images.cocodataset.org/annotations/annotations_trainval2017.zip')
            with open('annotations.zip', 'wb') as f: f.write(r.content)
            with zipfile.ZipFile('annotations.zip', 'r') as z: z.extractall('.')
            os.remove('annotations.zip')

        with open('annotations/captions_val2017.json', 'r') as f:
            coco_data = json.load(f)

        image_to_captions = {}
        for ann in coco_data['annotations']:
            img_id = ann['image_id']
            image_to_captions.setdefault(img_id, []).append(ann['caption'])

        added = 0
        for img_info in coco_data['images']:
            if added >= multimodal_samples: break
            img_id = img_info['id']
            img_file = f"val2017/{img_id:012d}.jpg"
            if os.path.exists(img_file) and img_id in image_to_captions:
                caption = random.choice(image_to_captions[img_id])
                try:
                    img = Image.open(img_file).convert('RGB').resize((cfg.model.image_size, cfg.model.image_size))
                    img_tensor = (torch.from_numpy(np.array(img)).permute(2, 0, 1).unsqueeze(0).float() / 255.0).to(self.device)
                    latent_bytes = self.ae.encode(img_tensor)
                    img_segment = pack_segment(cfg.model.img_start, latent_bytes, cfg.model.img_end)
                    tokens = [cfg.model.bos_id] + list("Describe this image:".encode('utf-8')) + img_segment + \
                             [cfg.model.assist_id] + list(caption.encode('utf-8')) + [cfg.model.eos_id]
                    self.data.append(torch.tensor(tokens, dtype=torch.long))
                    added += 1
                except:
                    pass

        # Text-only
        text_samples = max_samples - added
        try:
            resp = requests.get(cfg.data.pico_banana_jsonl_url, stream=True, timeout=30)
            for i, line in enumerate(resp.iter_lines()):
                if line and i < text_samples:
                    item = json.loads(line)
                    prompt = item.get('prompt', str(item))[:512]
                    completion = item.get('completion', "OK")[:512]
                    tokens = [cfg.model.bos_id] + list(prompt.encode('utf-8')) + [cfg.model.assist_id] + \
                             list(completion.encode('utf-8')) + [cfg.model.eos_id]
                    self.data.append(torch.tensor(tokens, dtype=torch.long))
        except Exception as e:
            logger.warning(f"pico-banana failed: {e}")
            for _ in range(text_samples):
                self.data.append(torch.randint(0, cfg.model.vocab_size, (256,)))

    def __len__(self): return len(self.data)
    def __getitem__(self, idx): return self.data[idx]

def collate_fn(batch, pad_token_id):
    max_len = max(len(inp) for inp in batch)
    padded = [torch.cat([inp, torch.full((max_len - len(inp),), pad_token_id, dtype=torch.long)]) for inp in batch]
    return torch.stack(padded)

# ====================== TRAINING ======================
def train(model, use_lora=False):
    # Pre-train AE so vision actually works
    logger.info("Pre-training Autoencoder (reconstruction)...")
    ae_opt = optim.Adam(model.ae.parameters(), lr=1e-3)
    for step in range(300):  # quick pretrain on COCO + noise
        if os.path.exists('val2017'):
            try:
                fname = random.choice(os.listdir('val2017')) if os.listdir('val2017') else None
                if fname and fname.endswith('.jpg'):
                    img = Image.open(f'val2017/{fname}').convert('RGB').resize((64,64))
                    x = (torch.from_numpy(np.array(img)).permute(2,0,1).unsqueeze(0).float() / 255.0).to(cfg.model.device)
                    ae_opt.zero_grad()
                    latent = model.ae.encoder(x)
                    recon = model.ae.decoder(latent)
                    loss = F.mse_loss(recon, x)
                    loss.backward()
                    ae_opt.step()
            except:
                pass
    logger.info("‚úÖ AE pre-trained")

    if use_lora:
        try:
            from peft import LoraConfig, get_peft_model
            lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=["qkv_proj","out_proj","w1","w2","w3","q_proj","kv_proj"],
                                     lora_dropout=0.05, bias="none")
            model = get_peft_model(model, lora_config)
            logger.info("LoRA enabled")
        except ImportError:
            logger.warning("peft not installed")

    dataset = AetherByteDataset(model)
    loader = DataLoader(dataset, batch_size=cfg.training.batch_size, collate_fn=lambda b: collate_fn(b, cfg.model.pad_id), shuffle=True)
    optimizer = optim.AdamW(model.parameters(), lr=cfg.training.learning_rate)
    scaler = GradScaler()

    for epoch in range(cfg.training.epochs):
        for batch in loader:
            batch = batch.to(cfg.model.device)
            optimizer.zero_grad()
            with autocast():
                _, loss, _ = model(batch, targets=batch)
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.grad_clip_norm)
            scaler.step(optimizer)
            scaler.update()
            logger.info(f"Epoch {epoch+1} Loss: {loss.item():.4f}")

    torch.save({'model': model.state_dict()}, os.path.join(cfg.training.checkpoint_dir, 'aether_byte_v3.1.pth'))
    print("‚úÖ Training complete")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', choices=['train', 'agent'], default='agent')
    parser.add_argument('--lora', action='store_true')
    args = parser.parse_args()

    os.makedirs(cfg.training.checkpoint_dir, exist_ok=True)
    os.makedirs("generated_images", exist_ok=True)

    model = ByteTransformer(cfg.model).to(cfg.model.device)
    if cfg.model.device == 'cuda':
        model = model.to(torch.bfloat16)

    ckpt_path = os.path.join(cfg.training.checkpoint_dir, 'aether_byte_v3.1.pth')
    if os.path.exists(ckpt_path):
        ckpt = torch.load(ckpt_path, map_location=cfg.model.device, weights_only=True)
        model.load_state_dict(ckpt['model'], strict=False)
        logger.info("‚úÖ Checkpoint loaded")

    if cfg.model.use_compile and torch.cuda.is_available():
        model = torch.compile(model, mode="reduce-overhead")
        logger.info("‚úÖ torch.compile activated")

    if args.mode == 'train':
        train(model, use_lora=args.lora)
    else:
        agent = AetherAgent(model)
        agent.run()

if __name__ == "__main__":
    main()